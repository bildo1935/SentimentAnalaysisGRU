{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56734fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import gensim\n",
    "import torchmetrics\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98862214",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CUDA_LAUNCH_BLOCKING=1\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available()==True else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d78ddf3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nontest_df = pd.read_csv(\"training.csv\")\n",
    "nontest_df = nontest_df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "010a1ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = list(nontest_df['body'])\n",
    "label = list(nontest_df['label'])\n",
    "total_data = list(zip(texts, label))\n",
    "\n",
    "for i in range(len(texts)):\n",
    "    # convert text to lowercase\n",
    "    text = texts[i].lower()\n",
    "    # word tokenizing\n",
    "    tokens = nltk.tokenize.word_tokenize(text)\n",
    "    # stemming tokens\n",
    "    stemmer = nltk.SnowballStemmer('english')\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "    texts[i] = tokens\n",
    "\n",
    "embedding_model = gensim.models.Word2Vec(sentences=texts, min_count=1, workers=5, window=3, sg=0, vector_size=100)\n",
    "embedding_model.save('w2v.model')\n",
    "word2index = {token: token_index for token_index, token in enumerate(embedding_model.wv.index_to_key)}\n",
    "index2word = {index: token for token, index in enumerate(embedding_model.wv.key_to_index)} \n",
    "        \n",
    "def text_preprocessing(text):\n",
    "    # convert text to lowercase\n",
    "    text = text.lower()\n",
    "    # word tokenizing\n",
    "    tokens = nltk.tokenize.word_tokenize(text)\n",
    "    # stemming tokens\n",
    "    stemmer = nltk.SnowballStemmer('english')\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "    tokens = [word2index[word] for word in tokens]\n",
    "    return tokens\n",
    "\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list = [], [] \n",
    "    for (_text, _label) in batch:\n",
    "        label_list.append(_label)\n",
    "        haha = text_preprocessing(_text)\n",
    "        processed_text = torch.LongTensor(haha)\n",
    "        text_list.append(processed_text)\n",
    "    label_list = torch.Tensor(label_list)\n",
    "    text_list = torch.nn.utils.rnn.pad_sequence(text_list, batch_first=True, padding_value=0)\n",
    "    text_list, label_list = text_list.to(device), label_list.to(device)\n",
    "    return text_list, label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f93872e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_split(train_size, val_size):\n",
    "    train_data = total_data[:train_size]\n",
    "    val_data = total_data[train_size:]\n",
    "    return train_data, val_data\n",
    "\n",
    "train_data, val_data = train_val_split(1950, 450)\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=10, collate_fn=collate_batch, shuffle=False)\n",
    "val_dataloader = DataLoader(val_data, batch_size=10, collate_fn=collate_batch, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3046855",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, num_classes, hidden_size, num_layers, batch_first, embedding_size):\n",
    "        super(TextClassifier, self).__init__()\n",
    "        self.RNN = nn.GRU(input_size=embedding_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=batch_first)\n",
    "        self.fc = nn.Linear(in_features=hidden_size, out_features=1)\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.FloatTensor(embedding_model.wv.vectors), padding_idx=0)\n",
    "        \n",
    "    def forward(self, x_in):\n",
    "        x_in = self.embedding(x_in)\n",
    "        _, y_out = self.RNN(x_in)\n",
    "        y_out = self.fc(y_out)\n",
    "        y_out = torch.squeeze(y_out, dim=0)\n",
    "        y_out = y_out.view(10)\n",
    "        return y_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "98b85d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimiser = torch.optim.SGD(TextClassifier(vocab_size=len(embedding_model.wv), num_classes=2, hidden_size=8, num_layers=1, batch_first=True, embedding_size=100).parameters(), lr=0.1, nesterov=True, momentum=0.9)\n",
    "loss_func = nn.BCEWithLogitsLoss()\n",
    "accuracy = torchmetrics.Accuracy(num_classes=1, threshold=0.5).to(device)\n",
    "#scheduler = torch.optim.lr_scheduler.ExponentialLR(optimiser, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d670617e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check\n",
      "tensor(0.7483, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>) tensor(0.7000)\n",
      "check\n",
      "tensor(0.8089, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>) tensor(0.7000)\n",
      "check\n",
      "tensor(1.0245, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>) tensor(0.2000)\n",
      "check\n",
      "tensor(0.7701, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>) tensor(0.7000)\n",
      "check\n",
      "tensor(0.9093, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>) tensor(0.2000)\n",
      "check\n",
      "tensor(0.6169, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>) tensor(0.7000)\n",
      "check\n",
      "tensor(0.6643, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>) tensor(0.7000)\n",
      "check\n",
      "tensor(0.7292, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>) tensor(0.7000)\n",
      "check\n",
      "tensor(0.7278, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>) tensor(0.7000)\n",
      "check\n",
      "tensor(0.6940, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>) tensor(0.7000)\n",
      "check\n",
      "tensor(0.6490, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>) tensor(0.7000)\n",
      "check\n",
      "tensor(0.8067, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>) tensor(0.7000)\n",
      "check\n",
      "tensor(0.6331, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>) tensor(0.7000)\n",
      "check\n",
      "tensor(0.8355, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>) tensor(0.7000)\n",
      "check\n",
      "tensor(0.7632, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>) tensor(0.7000)\n",
      "check\n",
      "tensor(0.6853, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>) tensor(0.7000)\n",
      "check\n",
      "tensor(0.6175, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>) tensor(0.7000)\n",
      "check\n",
      "tensor(1.0388, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>) tensor(0.3000)\n",
      "check\n",
      "tensor(0.6241, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>) tensor(0.7000)\n",
      "check\n",
      "tensor(0.7078, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>) tensor(0.7000)\n",
      "check\n",
      "tensor(0.8643, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>) tensor(0.3000)\n",
      "check\n",
      "tensor(0.6850, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>) tensor(0.7000)\n",
      "check\n",
      "tensor(0.6403, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>) tensor(0.7000)\n",
      "check\n",
      "tensor(0.6664, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>) tensor(0.7000)\n",
      "check\n",
      "tensor(0.6382, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>) tensor(0.7000)\n",
      "check\n",
      "tensor(0.8932, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>) tensor(0.2000)\n",
      "check\n",
      "tensor(0.6217, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>) tensor(0.7000)\n",
      "check\n",
      "tensor(0.6385, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>) tensor(0.7000)\n",
      "check\n",
      "tensor(0.6739, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>) tensor(0.7000)\n",
      "check\n",
      "tensor(0.7017, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>) tensor(0.7000)\n",
      "check\n",
      "tensor(0.9200, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>) tensor(0.3000)\n",
      "check\n",
      "tensor(0.6068, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>) tensor(0.7000)\n",
      "check\n",
      "tensor(0.6010, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>) tensor(0.7000)\n",
      "check\n",
      "tensor(0.6478, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>) tensor(0.7000)\n",
      "check\n",
      "tensor(0.7393, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>) tensor(0.7000)\n",
      "check\n",
      "tensor(0.6500, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>) tensor(0.7000)\n",
      "check\n",
      "tensor(0.6510, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>) tensor(0.8000)\n",
      "check\n",
      "tensor(0.8299, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>) tensor(0.7000)\n",
      "check\n",
      "tensor(0.5945, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>) tensor(0.7000)\n",
      "check\n",
      "tensor(0.7753, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>) tensor(0.7000)\n",
      "check\n",
      "tensor(0.7712, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>) tensor(0.8000)\n",
      "check\n",
      "tensor(0.6968, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>) tensor(0.7000)\n",
      "check\n",
      "tensor(0.7035, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>) tensor(0.7000)\n",
      "check\n",
      "tensor(0.6911, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>) tensor(0.7000)\n",
      "check\n",
      "tensor(0.5992, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>) tensor(0.7000)\n",
      "check\n",
      "tensor(0.7683, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>) tensor(0.7000)\n",
      "check\n",
      "tensor(0.7151, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>) tensor(0.8000)\n",
      "check\n",
      "tensor(0.6672, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>) tensor(0.7000)\n",
      "check\n",
      "tensor(0.6637, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>) tensor(0.7000)\n",
      "check\n",
      "tensor(0.8895, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>) tensor(0.2000)\n"
     ]
    }
   ],
   "source": [
    "#train\n",
    "\n",
    "epochs = 50\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for batch_text, batch_labels in train_dataloader:\n",
    "        optimiser.zero_grad()\n",
    "        machine = TextClassifier(vocab_size=len(embedding_model.wv), num_classes=2, hidden_size=8, num_layers=1, batch_first=True, embedding_size=100)\n",
    "        machine = machine.to(device)\n",
    "        y_pred = machine(x_in=batch_text)\n",
    "        loss = loss_func(y_pred, batch_labels.float())\n",
    "        acc = accuracy(preds=y_pred, target=batch_labels.long())\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(TextClassifier(vocab_size=len(embedding_model.wv), num_classes=2, hidden_size=1, num_layers=1, batch_first=True, embedding_size=100).parameters(), max_norm=1.0, norm_type=2.0, error_if_nonfinite=False)\n",
    "        optimiser.step()\n",
    "    print('check')\n",
    "    print(loss, acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3167e5e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7094, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>) tensor(0.6000)\n",
      "tensor(0.9338, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>) tensor(0.4000)\n",
      "tensor(0.7714, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>) tensor(0.6000)\n",
      "tensor(0.6611, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>) tensor(0.6000)\n",
      "tensor(0.7155, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>) tensor(0.6000)\n",
      "tensor(0.6855, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>) tensor(0.6000)\n",
      "tensor(1.1050, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>) tensor(0.4000)\n",
      "tensor(0.8106, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>) tensor(0.6000)\n",
      "tensor(0.6967, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>) tensor(0.6000)\n",
      "tensor(0.8038, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>) tensor(0.5000)\n",
      "tensor(0.6927, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>) tensor(0.6000)\n",
      "tensor(0.7491, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>) tensor(0.6000)\n",
      "tensor(0.6895, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>) tensor(0.6000)\n",
      "tensor(0.6717, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>) tensor(0.6000)\n",
      "tensor(0.7913, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>) tensor(0.6000)\n",
      "tensor(0.6663, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>) tensor(0.6000)\n",
      "tensor(1.1183, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>) tensor(0.4000)\n",
      "tensor(0.8184, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>) tensor(0.4000)\n",
      "tensor(0.9233, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>) tensor(0.4000)\n",
      "tensor(0.6543, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>) tensor(0.6000)\n",
      "tensor(0.6826, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>) tensor(0.6000)\n",
      "tensor(0.7711, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>) tensor(0.6000)\n",
      "tensor(0.6799, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>) tensor(0.6000)\n",
      "tensor(0.8172, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>) tensor(0.6000)\n",
      "tensor(0.9359, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>) tensor(0.4000)\n",
      "tensor(0.8374, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>) tensor(0.4000)\n",
      "tensor(1.1586, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>) tensor(0.4000)\n",
      "tensor(0.7229, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>) tensor(0.6000)\n",
      "tensor(0.6947, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>) tensor(0.6000)\n",
      "tensor(0.6591, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>) tensor(0.6000)\n",
      "tensor(0.7224, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>) tensor(0.6000)\n",
      "tensor(0.7870, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>) tensor(0.3000)\n",
      "tensor(0.8263, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>) tensor(0.6000)\n",
      "tensor(0.6757, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>) tensor(0.6000)\n",
      "tensor(0.6707, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>) tensor(0.6000)\n",
      "tensor(0.6731, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>) tensor(0.6000)\n",
      "tensor(0.7920, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>) tensor(0.6000)\n",
      "tensor(0.6453, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>) tensor(0.6000)\n",
      "tensor(0.6417, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>) tensor(0.7000)\n",
      "tensor(0.7751, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>) tensor(0.6000)\n",
      "tensor(0.7151, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>) tensor(0.6000)\n",
      "tensor(0.7016, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>) tensor(0.6000)\n",
      "tensor(1.0491, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>) tensor(0.4000)\n",
      "tensor(0.7510, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>) tensor(0.6000)\n",
      "tensor(0.8398, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>) tensor(0.4000)\n",
      "tensor(0.8331, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>) tensor(0.6000)\n",
      "tensor(0.7374, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>) tensor(0.6000)\n",
      "tensor(0.6724, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>) tensor(0.6000)\n",
      "tensor(0.6720, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>) tensor(0.6000)\n",
      "tensor(1.1837, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>) tensor(0.3000)\n"
     ]
    }
   ],
   "source": [
    "#val\n",
    "\n",
    "epoch = 25\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for batch_text, batch_labels in val_dataloader:\n",
    "        machine = TextClassifier(vocab_size=len(embedding_model.wv), num_classes=2, hidden_size=1, num_layers=1, batch_first=True, embedding_size=100)\n",
    "        machine = machine.to(device)\n",
    "        y_pred = machine(x_in=batch_text)\n",
    "        loss = loss_func(y_pred, batch_labels.float())\n",
    "        acc = accuracy(preds=y_pred, target=batch_labels.long())\n",
    "    print(loss, acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "562ae89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(machine, \"trained_model.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
